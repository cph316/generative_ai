{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMG6j3T1C7WQtbw5dEA7UsJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cph316/generative_ai/blob/main/%E3%80%90HW3%E3%80%91%E7%A0%94%E7%A9%B6GAN%E8%83%8C%E5%BE%8C%E5%8E%9F%E7%90%86_(Cross_Entropy%E3%80%81KL_divergence).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **主題二**"
      ],
      "metadata": {
        "id": "42o-F-ggWUWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 說明 Cross Entropy 與 KL Divergence\n",
        "\n",
        " Cross Entropy 和 KL Divergence 主要是用來衡量兩個機率分佈的差異。在 GAN中，這兩個概念很重要，因為生成器要學習讓它的分佈𝑄盡可能接近真實數據分佈𝑃，而判別器則在幫助區分。\n",
        "\n",
        "2. 公式與計算\n",
        "\n",
        "  (1) Cross Entropy\n",
        "\n",
        "  $H(𝑃,𝑄)=−i\\sum_{i}𝑃(𝑥_𝑖)log𝑄(𝑥_𝑖)$\n",
        "\n",
        "  其中：\n",
        "\n",
        "  $𝑃(𝑥_𝑖)$ 是真實分佈（通常是 one-hot 標籤）。\n",
        "\n",
        "  $𝑄(𝑥_𝑖)$ 是模型預測的機率分佈。\n",
        "\n",
        "  這代表「當我們使用𝑄來表達𝑃的時候，平均需要多少 bits 來表示這個系統的資訊？」\n",
        "\n",
        "  (2) KL Divergence\n",
        "  \n",
        "  $D_KL(P∥Q)=\\sum_{i}𝑃(𝑥_𝑖)logP(𝑥_𝑖)/​log𝑄(𝑥_𝑖)$\n",
        "\n",
        "  這代表「當我們用𝑄來代替𝑃，會額外產生多少錯誤資訊？」\n",
        "\n",
        "  (3) 兩者關係\n",
        "  $H(𝑃,𝑄)=H(𝑃)+D_KL(𝑃∥𝑄)$\n",
        "\n",
        "  -如果𝑄和𝑃完全相同，那麼 KL 散度為 0，交叉熵等於熵。\n",
        "  -但如果𝑄與𝑃差異很大，KL 散度會變大，交叉熵也會變大。\n",
        "\n"
      ],
      "metadata": {
        "id": "yVVFNMelqJsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 硬幣範例\n",
        "\n",
        " (1) 硬幣的真實分佈𝑃與預測分佈𝑄\n",
        " 假設：\n",
        "\n",
        " - 真實硬幣：正面出現的機率為 80%，反面為 20% →\n",
        "𝑃=[0.8,0.2]\n",
        "\n",
        " - 模型預測：模型認為正面是 60%，反面是 40% →\n",
        "𝑄=[0.6,0.4]\n",
        "\n",
        " 計算：\n",
        "\n",
        " - 交叉熵（Cross Entropy）：衡量模型預測的錯誤程度\n",
        "\n",
        " - KL 散度（KL Divergence）：衡量預測分佈與真實分佈的距離\n",
        "\n",
        " (2)Python 程式"
      ],
      "metadata": {
        "id": "JO5OOA7tutG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.special import rel_entr  # 計算 KL 散度\n",
        "\n",
        "# 真實分佈 P (真實硬幣機率)\n",
        "P = np.array([0.8, 0.2])\n",
        "\n",
        "# 預測分佈 Q (模型認為的硬幣機率)\n",
        "Q = np.array([0.6, 0.4])\n",
        "\n",
        "# 計算交叉熵 (Cross Entropy)\n",
        "cross_entropy = -np.sum(P * np.log(Q))\n",
        "print(f\"Cross Entropy: {cross_entropy:.4f}\")\n",
        "\n",
        "# 計算 KL 散度 (KL Divergence)\n",
        "kl_div = np.sum(rel_entr(P, Q))  # rel_entr(a, b) = a * log(a / b)\n",
        "print(f\"KL Divergence: {kl_div:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k10BixjVUp3o",
        "outputId": "8d19cb1d-91f8-4ba8-f5e1-dbba22086e15"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Entropy: 0.5919\n",
            "KL Divergence: 0.0915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " (3) 執行結果\n",
        " - 交叉熵（0.5919）：代表模型預測的機率分佈與真實標籤的誤差，如果𝑄越接近𝑃，交叉熵就會越小。\n",
        "\n",
        " - KL 散度（0.0915）：表示預測分佈與真實分佈的距離，數值越小代表模型學得越好。"
      ],
      "metadata": {
        "id": "tKba5116U2-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**結論**\n",
        "\n",
        "以上範例可以更直觀理解：\n",
        "\n",
        "- 交叉熵 用於訓練模型，幫助它學習接近真實分佈。\n",
        "\n",
        "- KL 散度 是衡量兩個機率分佈之間的距離，數值越大代表模型預測越不準確。\n",
        "\n",
        "這也與 **GAN 的生成器訓練** 有關，因為生成器希望讓\n",
        "𝑄盡可能接近𝑃，這樣判別器就無法分辨真假數據，從而達到更好的生成效果"
      ],
      "metadata": {
        "id": "dxejaW9RVOic"
      }
    }
  ]
}